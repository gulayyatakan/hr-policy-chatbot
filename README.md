HR Policy Chatbot

This project implements a Retrieval-Augmented Generation (RAG) chatbot that answers questions about HR policies such as vacation rules, celebrations, remote work, and other handbook topics.

The system consists of:
    â€¢    a FastAPI backend
    â€¢    a ChromaDB vector store for document retrieval
    â€¢    a local language model served by Ollama (phi3:mini by default)
    â€¢    a React/Vite frontend (frontend_vite/) providing a modern chat interface

All HR policy source documents are stored as Markdown files under data/hr_policies/.



Features
    â€¢    Markdown ingestion
    â€¢    Script src/ingest_md.py scans all .md files under data/hr_policies/ (including subfolders).
    â€¢    Files are split into semantic chunks.
    â€¢    Each chunk is embedded with sentence-transformers/all-MiniLM-L6-v2.
    â€¢    Embeddings and metadata (source path + chunk index) are stored in a persistent ChromaDB collection named hr-policies.
    â€¢    Vector retrieval
    â€¢    At query time the backend converts an incoming question into an embedding.
    â€¢    The most similar chunks are retrieved from the ChromaDB collection.
    â€¢    Retrieved chunks are used as the context for answer generation.
    â€¢    Local answer generation (Ollama)
    â€¢    Context and question are combined into a prompt.
    â€¢    A local LLM served by Ollama (phi3:mini by default) generates the answer.
    â€¢    The prompt instructs the model to answer only from the provided context and to return a fallback message if the information is not present.
    â€¢    Streaming endpoint
    â€¢    Besides a standard /chat endpoint, the backend provides /chat-stream, which streams the answer token-by-token as plain text.
    â€¢    The Vite frontend consumes this stream and progressively updates the last assistant message, improving perceived latency.
    â€¢    Modern frontend
    â€¢    The frontend_vite/ application uses React + Vite + TailwindCSS.
    â€¢    The UI consists of a header, a main chat area, and an autosizing input field.
    â€¢    Messages are rendered with Markdown, loading states and errors are visualised, and the view auto-scrolls while streaming.



Project Structure

hr_rag_chatbot_new/
â”œâ”€â”€ config.py                â€“ optional global configuration (paths, model names)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ hr_policies/         â€“ HR policy Markdown files (subfolders allowed)
â”‚   â””â”€â”€ chroma/              â€“ persistent ChromaDB index (created by ingest_md.py)
â”œâ”€â”€ frontend_vite/           â€“ Vite/React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api.js           â€“ wrapper around backend API (/chat and /chat-stream)
â”‚   â”‚   â”œâ”€â”€ App.jsx          â€“ main layout (header + Chatbot)
â”‚   â”‚   â”œâ”€â”€ components/      â€“ Chatbot, ChatInput, ChatMessages, Spinner
â”‚   â”‚   â”œâ”€â”€ hooks/           â€“ custom hooks (autosize textarea, auto-scroll, etc.)
â”‚   â”‚   â””â”€â”€ index.css        â€“ Tailwind and base styles
â”‚   â””â”€â”€ â€¦                    â€“ Vite config, assets, package.json, etc.
â”œâ”€â”€ hr_docs/                 â€“ optional scratch directory (not required)
â”œâ”€â”€ requirements.txt         â€“ Python backend dependencies
â””â”€â”€ src/
â”œâ”€â”€ api_server.py        â€“ FastAPI app exposing /chat and /chat-stream
â”œâ”€â”€ ingest_md.py         â€“ ingestion script for HR Markdown policies
â”œâ”€â”€ rag_backend.py       â€“ earlier CLI retrieval script (debugging)
â”œâ”€â”€ rag_chat_ollama.py   â€“ earlier terminal chat with Ollama + RAG
â””â”€â”€ chatbot.py           â€“ simple local Ollama chat (no RAG)

â¸»

Prerequisites

The following components are required:
    â€¢    Python 3.11
    â€¢    Backend components rely on packages that currently support Python 3.11 (for example onnxruntime via ChromaDB).
    â€¢    Node.js â‰¥ 16 and npm
    â€¢    Required for building and running the React/Vite frontend.
    â€¢    Ollama
    â€¢    Local LLM runtime used for answer generation.
    â€¢    On macOS with Homebrew, a typical installation looks as follows:
    â€¢    brew install ollama
    â€¢    ollama pull phi3:mini
    â€¢    Any other Ollama model can be used; the model name just needs to be updated in src/api_server.py.



Backend Setup

1. Create and activate virtual environment

From the project root hr_rag_chatbot_new/:
    â€¢    python3.11 -m venv .venv
    â€¢    source .venv/bin/activate

After activation, python --version should report a 3.11.x release.

2. Install Python dependencies

With the virtual environment active:
    â€¢    pip install --upgrade pip
    â€¢    pip install -r requirements.txt

This installs FastAPI, Uvicorn, ChromaDB, sentence-transformers, and the Ollama client, among others.



Populate the Knowledge Base

3. Place HR policy documents

Store the HR Markdown files under data/hr_policies/.
Subdirectories are supported, for example:

data/hr_policies/
â”œâ”€â”€ vacation/
â”‚   â””â”€â”€ gitlab_vacation_policy.md
â”œâ”€â”€ celebrations/
â”‚   â””â”€â”€ celebrations.md
â””â”€â”€ other-topics/
â””â”€â”€ â€¦

4. Run the ingestion script

With the virtual environment active:
    â€¢    python src/ingest_md.py

The script:
    â€¢    scans all .md files under data/hr_policies/
    â€¢    splits content into chunks
    â€¢    computes embeddings with all-MiniLM-L6-v2
    â€¢    upserts the chunks and metadata into the ChromaDB collection hr-policies under data/chroma/

The script logs the number of documents and chunks processed and confirms when ingestion has completed successfully.



Run the Backend API

5. Ensure Ollama and the model are running

Typical sequence:
    â€¢    start the Ollama service if necessary: ollama serve
    â€¢    ensure the model is available: ollama pull phi3:mini

6. Start the FastAPI server

With the virtual environment active:
    â€¢    uvicorn src.api_server:app --reload --port 8000

The server listens on http://127.0.0.1:8000 and exposes:
    â€¢    POST /chat
    â€¢    Request body: JSON object with a question field
    â€¢    example: { "question": "How many vacation days do I have at GitLab?" }
    â€¢    Response body: JSON object containing
    â€¢    answer: generated answer text
    â€¢    sources: list of { "source": <path>, "chunk_index": <int> } used for the answer
    â€¢    POST /chat-stream
    â€¢    Same request body as /chat
    â€¢    Returns a streaming response (Server-Sent Events style, just plain text chunks) representing the answer as it is generated.
    â€¢    Intended for use by the frontend to display incremental updates.

A quick manual test from the terminal can be done with:
    â€¢    curl -X POST "http://127.0.0.1:8000/chat" -H "Content-Type: application/json" -d '{"question": "How many vacation days do I have at GitLab?"}'

If ingestion and Ollama are configured correctly, the response contains the correct number of vacation days and corresponding sources.


Frontend Setup (Vite + React)

The Vite frontend resides in frontend_vite/.

7. Install Node dependencies

From frontend_vite/:
    â€¢    npm install

8. Configure backend URL

Create a .env.local file in frontend_vite/ with the backend base URL:
    â€¢    VITE_API_URL=http://127.0.0.1:8000

The frontend reads this environment variable in src/api.js to know where to send chat requests.

9. Start the frontend development server

From frontend_vite/:
    â€¢    npm run dev

By default Vite serves the app at http://localhost:3000.
The page displays the header plus the chat interface.


Frontend Chat Behaviour

The main chat logic is implemented in frontend_vite/src/components/Chatbot.jsx, together with helper components ChatMessages.jsx and ChatInput.jsx.

Typical flow:
    1.    When the application loads and no messages exist yet, an introductory block is displayed:
    â€¢    â€œğŸ‘‹ Welcome!â€
    â€¢    Explanation that the assistant answers questions purely based on HR handbook documents (vacation, celebrations, remote work, etc.).
    2.    After a question is entered in the input field and submitted:
    â€¢    A user message is appended to the internal state.
    â€¢    An empty assistant message with loading: true is added.
    3.    The Chatbot component uses api.js to communicate with the backend:
    â€¢    api.createChat() can initialise a chat session if required by the API design.
    â€¢    api.sendChatMessage(chatId, message) sends the question and receives a text stream from /chat-stream.
    4.    The parseSSEStream helper (src/utils.js) iterates over the server-sent text chunks.
For each chunk, the assistantâ€™s last message is updated by appending the new text.
    5.    When streaming completes, the assistant message is marked as loading: false.
Errors during the process mark the message with error: true and show an error line, while keeping the existing content if any.



Customisation (current scope)
    â€¢    Introductory text can be adapted in frontend_vite/src/components/Chatbot.jsx by editing the JSX block that is rendered when messages.length === 0.
    â€¢    The system prompt and behaviour of the LLM can be tuned in src/api_server.py inside the functions that build the context and call ollama.chat.
    â€¢    For example, the strictness regarding â€œanswer only from contextâ€ or the tone of the answer can be adjusted here.
    â€¢    The embedding model can be changed by modifying the EMBEDDING_MODEL_NAME constant in src/api_server.py.
Any model supported by sentence-transformers and compatible with the current hardware can be used.



Example Questions

After backend and frontend are running and the GitLab HR policies are ingested, typical interactions include:
    â€¢    â€œHow many vacation days do I have at GitLab?â€
    â€¢    Answer: number of days per year as specified in the vacation policy.
    â€¢    â€œCan unused vacation days be carried over to the next year?â€
    â€¢    Answer: explanation of carry-over rules and relevant deadline (for example 31 March).
    â€¢    â€œWhat happens if my birthday falls on a weekend?â€
    â€¢    Answer: description of taking an alternative day off according to the celebrations policy.
    â€¢    â€œIs there a policy about celebrations or significant life events?â€
    â€¢    Answer: overview of relevant sections in the celebrations or people-related policies.

For unrelated questions such as â€œRecommend the best burger place in New Yorkâ€ the assistant responds with a variant of:
    â€¢    â€œI cannot answer that based on the available HR policies.â€



MVP Scope

For the current semester project, the minimal viable product (MVP) includes:
    â€¢    ingestion of HR policies from Markdown files into a vector store
    â€¢    retrieval of relevant policy chunks for a given question
    â€¢    generation of answers using a local Ollama model, constrained to the retrieved context
    â€¢    exposure of /chat and /chat-stream HTTP endpoints via FastAPI
    â€¢    a working React/Vite frontend that:
    â€¢    sends questions to the backend
    â€¢    streams and displays answers
    â€¢    shows a clear introductory message and basic error states

This MVP demonstrates an end-to-end RAG pipeline over HR policies with a locally hosted LLM and a modern web interface.


License

The project is released under the MIT License.
Details are provided in the LICENSE file.
